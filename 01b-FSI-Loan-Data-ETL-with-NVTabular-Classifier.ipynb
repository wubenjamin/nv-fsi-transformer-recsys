{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7783917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ======================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6b360",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_transformers4rec_getting-started-session-based-01-etl-with-nvtabular/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# ETL with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6085c0",
   "metadata": {},
   "source": [
    "In this notebook we are going to generate synthetic data and then create sequential features with [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular). Such data will be used in the next notebook to train a session-based recommendation model.\n",
    "\n",
    "NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems. It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add26d16",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8dae24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/cudf/utils/gpu_utils.py:148: UserWarning: No NVIDIA GPU detected\n",
      "  warnings.warn(\"No NVIDIA GPU detected\")\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'\n",
      "  warn(f\"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import glob\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "from merlin.schema.tags import Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3206b3f",
   "metadata": {},
   "source": [
    "### Define Input/Output Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "105dd71c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/workspace/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36498a01",
   "metadata": {},
   "source": [
    "## Load FSI Synthetic Demo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929036ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define paths to the FSI synthetic demo data files\n",
    "DATA_FILES = [\n",
    "    os.path.join(INPUT_DATA_DIR, \"synthetic_fsi\", \"Synthetic_Demo_Data_Shared_Part1.xlsx\"),\n",
    "    os.path.join(INPUT_DATA_DIR, \"synthetic_fsi\", \"Synthetic_Demo_Data_Shared_Part2.xlsx\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c65dfd-0cf7-4a5b-9383-4c10b50fd136",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from parquet file: /workspace/data/synthetic_fsi/synthetic_demo_data.parquet\n",
      "Loaded 440787 rows\n",
      "Total rows: 440787\n",
      "Available columns: ['session_date', 'loan_id', 'has_mobile_app', 'debtiq_enrolled', 'pa_eligible', 'topup_eligible', 'ita_eligible', 'email_sent_in_last_90_days', 'dm_sent_in_last_90_days', 'fico', 'income_', 'existing_loan_size_', 'current_loan_mob', 'offer___carousel', 'servicing___carousel', 'feature_sheet', 'bottom_sheet', 'converts_for_a_topup']\n"
     ]
    }
   ],
   "source": [
    "# Check if parquet file exists\n",
    "parquet_path = os.path.join(INPUT_DATA_DIR, \"synthetic_fsi\", \"synthetic_demo_data.parquet\")\n",
    "\n",
    "if os.path.exists(parquet_path):\n",
    "    print(f\"Loading data from parquet file: {parquet_path}\")\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "\n",
    "else:\n",
    "    print(\"Parquet file not found. Loading from Excel files...\")\n",
    "    dfs = []\n",
    "    for file_path in DATA_FILES:\n",
    "        print(f\"Reading {file_path}...\")\n",
    "        sheet_name = \"Data_Part1\" if \"Part1\" in file_path else \"Data_Part2\"\n",
    "        df_temp = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1)  # Read from appropriate sheet\n",
    "        # Take first 100 rows for testing\n",
    "        # df_temp = df_temp.head(100)\n",
    "        # Clean column names to be more pythonic\n",
    "        df_temp.columns = [col.strip().lower()\n",
    "                          .replace(' ', '_')\n",
    "                          .replace('(', '')\n",
    "                          .replace(')', '')\n",
    "                          .replace('$', '')\n",
    "                          .replace('-', '_')\n",
    "                          for col in df_temp.columns]\n",
    "        dfs.append(df_temp)\n",
    "        print(f\"Loaded {len(df_temp)} rows from {os.path.basename(file_path)}\")\n",
    "\n",
    "    # Combine all dataframes\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Save to parquet for faster loading next time\n",
    "    print(f\"Saving data to parquet file: {parquet_path}\")\n",
    "    os.makedirs(os.path.dirname(parquet_path), exist_ok=True)\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Available columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d0890-dabb-4821-bd39-101141669935",
   "metadata": {},
   "source": [
    "Visualize couple of rows of the loaded FSI synthetic demo dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc4d884-eab6-4e7c-8570-61d24d066710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_date</th>\n",
       "      <th>loan_id</th>\n",
       "      <th>has_mobile_app</th>\n",
       "      <th>debtiq_enrolled</th>\n",
       "      <th>pa_eligible</th>\n",
       "      <th>topup_eligible</th>\n",
       "      <th>ita_eligible</th>\n",
       "      <th>email_sent_in_last_90_days</th>\n",
       "      <th>dm_sent_in_last_90_days</th>\n",
       "      <th>fico</th>\n",
       "      <th>income_</th>\n",
       "      <th>existing_loan_size_</th>\n",
       "      <th>current_loan_mob</th>\n",
       "      <th>offer___carousel</th>\n",
       "      <th>servicing___carousel</th>\n",
       "      <th>feature_sheet</th>\n",
       "      <th>bottom_sheet</th>\n",
       "      <th>converts_for_a_topup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>4954838</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>807</td>\n",
       "      <td>57422</td>\n",
       "      <td>10857</td>\n",
       "      <td>9</td>\n",
       "      <td>Topup</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>4765835</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>741</td>\n",
       "      <td>63181</td>\n",
       "      <td>9287</td>\n",
       "      <td>28</td>\n",
       "      <td>Topup</td>\n",
       "      <td>Topup</td>\n",
       "      <td>blank</td>\n",
       "      <td>ITA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>4185554</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>745</td>\n",
       "      <td>50730</td>\n",
       "      <td>9720</td>\n",
       "      <td>28</td>\n",
       "      <td>Topup</td>\n",
       "      <td>Topup</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>7019817</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>741</td>\n",
       "      <td>148623</td>\n",
       "      <td>10786</td>\n",
       "      <td>12</td>\n",
       "      <td>Topup</td>\n",
       "      <td>ITA</td>\n",
       "      <td>blank</td>\n",
       "      <td>ITA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-24</td>\n",
       "      <td>8168610</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>752</td>\n",
       "      <td>110899</td>\n",
       "      <td>11537</td>\n",
       "      <td>27</td>\n",
       "      <td>Topup</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  session_date  loan_id  has_mobile_app  debtiq_enrolled  pa_eligible  \\\n",
       "0   2025-05-22  4954838               1                1            1   \n",
       "1   2025-05-21  4765835               1                1            1   \n",
       "2   2025-05-21  4185554               0                0            1   \n",
       "3   2025-05-21  7019817               0                0            1   \n",
       "4   2025-05-24  8168610               0                0            1   \n",
       "\n",
       "   topup_eligible  ita_eligible  email_sent_in_last_90_days  \\\n",
       "0               1             1                          17   \n",
       "1               1             1                          24   \n",
       "2               1             1                          20   \n",
       "3               1             1                          17   \n",
       "4               1             1                          19   \n",
       "\n",
       "   dm_sent_in_last_90_days  fico  income_  existing_loan_size_  \\\n",
       "0                        2   807    57422                10857   \n",
       "1                        1   741    63181                 9287   \n",
       "2                        2   745    50730                 9720   \n",
       "3                        1   741   148623                10786   \n",
       "4                        1   752   110899                11537   \n",
       "\n",
       "   current_loan_mob offer___carousel servicing___carousel feature_sheet  \\\n",
       "0                 9            Topup                  ITA           ITA   \n",
       "1                28            Topup                Topup         blank   \n",
       "2                28            Topup                Topup           ITA   \n",
       "3                12            Topup                  ITA         blank   \n",
       "4                27            Topup                  ITA           ITA   \n",
       "\n",
       "  bottom_sheet  converts_for_a_topup  \n",
       "0          ITA                     0  \n",
       "1          ITA                     0  \n",
       "2          ITA                     0  \n",
       "3          ITA                     0  \n",
       "4          ITA                     0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae36e04",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139de226",
   "metadata": {},
   "source": [
    "Deep Learning models require dense input features. Categorical features are sparse, and need to be represented by dense embeddings in the model. To allow for that, categorical features first need to be encoded as contiguous integers `(0, ..., |C|)`, where `|C|` is the feature cardinality (number of unique values), so that their embeddings can be efficiently stored in embedding layers.  We will use NVTabular to preprocess the categorical features, so that all categorical columns are encoded as contiguous integers. Note that the `Categorify` op encodes `nulls` to `1`, OOVs to `2` automatically. We preserve `0` for padding. The encoding of other categories starts from `3`. In our FSI demo dataset we handle any nulls appropriately. On the other hand `0` is used for padding the sequences in input block. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3bb9c",
   "metadata": {},
   "source": [
    "Here our goal is to create sequential features. To do so, we are grouping the features together at the session level in the following cell. In this FSI demo dataset, we may not have a timestamp column, but if we had one (that's the case for most real-world datasets), we would be sorting the interactions by the timestamp column as in this [example notebook](../end-to-end-session-based/01-ETL-with-NVTabular.ipynb). Note that we also trim each feature sequence in a  session to a certain length. Here, we use the NVTabular library so that we can easily preprocess and create features on GPU with a few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b7d9da9-6b0f-4295-8f77-6191a6a966cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing FSI data for sequential recommendation modeling...\n",
      "Unique product interactions: 7\n",
      "Applying NVTabular workflow to create sequential features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/io/dataset.py:264: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 154\u001b[0m\n\u001b[1;32m    151\u001b[0m dataset \u001b[38;5;241m=\u001b[39m nvt\u001b[38;5;241m.\u001b[39mDataset(df)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Generate statistics for the features and export parquet files\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(INPUT_DATA_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_nvt_classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Sequential feature engineering completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 Features created for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m loan interactions\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nvtabular/workflow/workflow.py:264\u001b[0m, in \u001b[0;36mWorkflow.fit_transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: Dataset) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    245\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convenience method to both fit the workflow and transform the dataset in a single\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    call. Equivalent to calling ``workflow.fit(dataset)`` followed by\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    ``workflow.transform(dataset)``\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;03m    transform\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(dataset)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nvtabular/workflow/workflow.py:228\u001b[0m, in \u001b[0;36mWorkflow.fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m current_phase:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# this shouldn't happen, but lets not infinite loop just in case\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed to find dependency-free StatOperator to fit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_phase\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# Remove all the operators we processed in this phase, and remove\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# from the dependencies of other ops too\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m current_phase:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/merlin/dag/executors.py:369\u001b[0m, in \u001b[0;36mDaskExecutor.fit\u001b[0;34m(self, ddf, nodes, strict)\u001b[0m\n\u001b[1;32m    360\u001b[0m transformed_ddf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\n\u001b[1;32m    361\u001b[0m     ddf,\n\u001b[1;32m    362\u001b[0m     node\u001b[38;5;241m.\u001b[39mparents_with_dependencies,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m     strict\u001b[38;5;241m=\u001b[39mstrict,\n\u001b[1;32m    366\u001b[0m )\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     stats\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_ddf\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    371\u001b[0m     LOG\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fit operator \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, node\u001b[38;5;241m.\u001b[39mop)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nvtabular/ops/value_counts.py:44\u001b[0m, in \u001b[0;36mValueCount.fit\u001b[0;34m(self, col_selector, ddf)\u001b[0m\n\u001b[1;32m     40\u001b[0m stats[col] \u001b[38;5;241m=\u001b[39m stats[col] \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m stats \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m     41\u001b[0m stats[col][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     42\u001b[0m     {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_count\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stats[col] \u001b[38;5;28;01melse\u001b[39;00m stats[col][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue_count\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m offs \u001b[38;5;241m=\u001b[39m pull_apart_list(\u001b[43mseries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     45\u001b[0m lh, rh \u001b[38;5;241m=\u001b[39m offs[\u001b[38;5;241m1\u001b[39m:], offs[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     46\u001b[0m rh \u001b[38;5;241m=\u001b[39m rh\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    292\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/base.py:600\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_keys__())\n\u001b[1;32m    598\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 600\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pool, multiprocessing\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mPool):\n\u001b[1;32m     87\u001b[0m         pool \u001b[38;5;241m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[0;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mget_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_get_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpack_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_exception\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwaiting\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, res_info, failed \u001b[38;5;129;01min\u001b[39;00m \u001b[43mqueue_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[38;5;241m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/dask/local.py:137\u001b[0m, in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mqueue_get\u001b[39m(q):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/queue.py:170\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First, let's prepare the FSI data for sequential modeling\n",
    "print(\"Preparing FSI data for sequential recommendation modeling...\")\n",
    "\n",
    "# Convert session_date to datetime and extract day number for temporal features\n",
    "df['session_date'] = pd.to_datetime(df['session_date'])\n",
    "df['day'] = (df['session_date'] - df['session_date'].min()).dt.days + 1\n",
    "\n",
    "# Create a product interaction sequence - combining the carousel and sheet features\n",
    "# This represents the sequence of financial products/offers shown to each loan customer\n",
    "df['product_interaction'] = df['offer___carousel'].astype(str) + '_' + df['servicing___carousel'].astype(str)\n",
    "\n",
    "# For this FSI use case, we'll treat each unique product interaction as an \"item\"\n",
    "# and create sequences of these interactions per loan_id\n",
    "print(f\"Unique product interactions: {df['product_interaction'].nunique()}\")\n",
    "\n",
    "SESSIONS_MAX_LENGTH = 10  # Reduced for financial data which typically has shorter sequences\n",
    "\n",
    "# Define categorical features to encode\n",
    "categorical_features = [\n",
    "    'product_interaction',  # Our main \"item\" - combinations of offers/services\n",
    "    'offer___carousel',     # Individual offer type\n",
    "    'servicing___carousel', # Individual service type  \n",
    "    'feature_sheet',        # Feature sheet shown\n",
    "    'bottom_sheet'          # Bottom sheet type\n",
    "]\n",
    "\n",
    "# Define continuous/numerical features\n",
    "continuous_features = [\n",
    "    'fico',                        # Credit score\n",
    "    'income_',                     # Customer income\n",
    "    'existing_loan_size_',         # Current loan amount\n",
    "    'current_loan_mob',            # Months on book\n",
    "    'email_sent_in_last_90_days',  # Email frequency\n",
    "    'dm_sent_in_last_90_days'      # Direct mail frequency\n",
    "]\n",
    "\n",
    "# Define binary features (treat as categorical for embedding)\n",
    "binary_features = [\n",
    "    'has_mobile_app',\n",
    "    'debtiq_enrolled', \n",
    "    'pa_eligible',\n",
    "    'topup_eligible',\n",
    "    'ita_eligible'\n",
    "]\n",
    "\n",
    "# Categorify all categorical and binary features\n",
    "categ_feats = (categorical_features + binary_features) >> nvt.ops.Categorify()\n",
    "\n",
    "# Prepare continuous features \n",
    "cont_feats = continuous_features >> nvt.ops.FillMissing() >> nvt.ops.Normalize()\n",
    "\n",
    "# Define the complete feature set for groupby\n",
    "groupby_feats = categ_feats + cont_feats + ['loan_id', 'day', 'converts_for_a_topup']\n",
    "\n",
    "# Group features by loan_id to create sequences\n",
    "groupby_features = groupby_feats >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"loan_id\"], \n",
    "    aggs={\n",
    "        # Main product interaction sequence (our \"items\")\n",
    "        \"product_interaction\": [\"list\", \"count\"],\n",
    "        # Other categorical sequences\n",
    "        \"offer___carousel\": [\"list\"],\n",
    "        \"servicing___carousel\": [\"list\"], \n",
    "        \"feature_sheet\": [\"list\"],\n",
    "        \"bottom_sheet\": [\"list\"],\n",
    "        # Binary feature sequences\n",
    "        \"has_mobile_app\": [\"list\"],\n",
    "        \"debtiq_enrolled\": [\"list\"],\n",
    "        \"pa_eligible\": [\"list\"], \n",
    "        \"topup_eligible\": [\"list\"],\n",
    "        \"ita_eligible\": [\"list\"],\n",
    "        # Continuous feature sequences\n",
    "        \"fico\": [\"list\"],\n",
    "        \"income_\": [\"list\"],\n",
    "        \"existing_loan_size_\": [\"list\"],\n",
    "        \"current_loan_mob\": [\"list\"],\n",
    "        \"email_sent_in_last_90_days\": [\"list\"],\n",
    "        \"dm_sent_in_last_90_days\": [\"list\"],\n",
    "        # Temporal and target features\n",
    "        \"day\": [\"first\"],\n",
    "        \"converts_for_a_topup\": [\"max\"]  # Single target value for binary classification\n",
    "        },\n",
    "    name_sep=\"-\")\n",
    "\n",
    "# Create main item sequence (product interactions) with proper tagging\n",
    "sequence_features_item = (\n",
    "    groupby_features['product_interaction-list']\n",
    "    >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH) \n",
    "    >> TagAsItemID()\n",
    ")\n",
    "\n",
    "# Create categorical feature sequences\n",
    "categorical_sequences = (\n",
    "    groupby_features['offer___carousel-list', 'servicing___carousel-list', \n",
    "                    'feature_sheet-list', 'bottom_sheet-list'] \n",
    "    >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH)\n",
    ")\n",
    "\n",
    "# Create binary feature sequences \n",
    "binary_sequences = (\n",
    "    groupby_features['has_mobile_app-list', 'debtiq_enrolled-list', 'pa_eligible-list',\n",
    "                    'topup_eligible-list', 'ita_eligible-list']\n",
    "    >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH)\n",
    "    >> nvt.ops.AddMetadata(tags=[Tags.CATEGORICAL])\n",
    ")\n",
    "\n",
    "# Create continuous feature sequences\n",
    "continuous_sequences = (\n",
    "    groupby_features['fico-list', 'income_-list', 'existing_loan_size_-list',\n",
    "                    'current_loan_mob-list', 'email_sent_in_last_90_days-list',\n",
    "                    'dm_sent_in_last_90_days-list'] \n",
    "    >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH)\n",
    "    >> nvt.ops.AddMetadata(tags=[Tags.CONTINUOUS])\n",
    ")\n",
    "\n",
    "# Create target feature (single value for binary classification)\n",
    "target_feature = (\n",
    "    groupby_features['converts_for_a_topup-max']\n",
    "    >> nvt.ops.AddMetadata(tags=[Tags.TARGET])\n",
    ")\n",
    "\n",
    "# Filter out loans with very short interaction sequences\n",
    "MINIMUM_SESSION_LENGTH = 1  # Keep all loans since FSI data may have single interactions\n",
    "selected_features = (\n",
    "    groupby_features['product_interaction-count', 'day-first', 'loan_id'] + \n",
    "    sequence_features_item +\n",
    "    categorical_sequences + \n",
    "    binary_sequences +\n",
    "    continuous_sequences +\n",
    "    target_feature\n",
    ")\n",
    "\n",
    "# For FSI data, we'll keep all loans even with single interactions    \n",
    "filtered_sessions = selected_features\n",
    "\n",
    "# Create final feature list with value counts for embedding dimensions\n",
    "seq_feats_list = filtered_sessions[\n",
    "    'product_interaction-list', 'offer___carousel-list', 'servicing___carousel-list',\n",
    "    'feature_sheet-list', 'bottom_sheet-list', 'has_mobile_app-list', \n",
    "    'debtiq_enrolled-list', 'pa_eligible-list', 'topup_eligible-list', \n",
    "    'ita_eligible-list', 'fico-list', 'income_-list', 'existing_loan_size_-list',\n",
    "    'current_loan_mob-list', 'email_sent_in_last_90_days-list', \n",
    "    'dm_sent_in_last_90_days-list', 'converts_for_a_topup-max'\n",
    "] >> nvt.ops.ValueCount()\n",
    "\n",
    "# Create the complete workflow\n",
    "workflow = nvt.Workflow(filtered_sessions['loan_id', 'day-first'] + seq_feats_list)\n",
    "\n",
    "# Apply the workflow to create sequential features\n",
    "print(\"Applying NVTabular workflow to create sequential features...\")\n",
    "dataset = nvt.Dataset(df)\n",
    "\n",
    "# Generate statistics for the features and export parquet files\n",
    "workflow.fit_transform(dataset).to_parquet(os.path.join(INPUT_DATA_DIR, \"processed_nvt_classifier\"))\n",
    "\n",
    "print(\"✅ Sequential feature engineering completed!\")\n",
    "print(f\"📊 Features created for {len(df)} loan interactions\")\n",
    "print(f\"🎯 Target variable: converts_for_a_topup (max value for binary classification)\")\n",
    "print(f\"📝 Main item sequence: product_interaction (combinations of offers/services)\")\n",
    "print(f\"🔄 Note: This version is optimized for BinaryClassificationTask with single target values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458c28f",
   "metadata": {},
   "source": [
    "It is possible to save the preprocessing workflow. That is useful to apply the same preprocessing to other data (with the same schema) and also to deploy the session-based recommendation pipeline to Triton Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e42cbf-edd6-44af-af23-c026edb578c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.output_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b54bb-4549-49a3-89bb-1f573a426aca",
   "metadata": {},
   "source": [
    "Save NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f498dce-69eb-4f88-8ddd-8629558825df",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save(os.path.join(INPUT_DATA_DIR, \"workflow_etl_classifier\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a41961",
   "metadata": {},
   "source": [
    "## Export pre-processed data by day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cedca3",
   "metadata": {},
   "source": [
    "In this example we are going to split the preprocessed parquet files by days, to allow for temporal training and evaluation. There will be a folder for each day and three parquet files within each day folder: `train.parquet`, `validation.parquet` and `test.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3e59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\",os.path.join(INPUT_DATA_DIR, \"sessions_by_day_classifier\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fb27a-0c64-43eb-be79-42213944990b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the processed parquet file\n",
    "sessions_gdf = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, \"processed_nvt_classifier/part_0.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537a248-059e-4db9-8b62-9681175f0193",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(sessions_gdf.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67a92b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers4rec.utils.data_utils import save_time_based_splits\n",
    "save_time_based_splits(data=nvt.Dataset(sessions_gdf),\n",
    "                       output_dir= OUTPUT_DIR,\n",
    "                       partition_col='day-first',\n",
    "                       timestamp_col='loan_id', \n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72337b",
   "metadata": {},
   "source": [
    "## Check out the preprocessed outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd04ec82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAIN_PATHS = os.path.join(OUTPUT_DIR, \"1\", \"train.parquet\")\n",
    "TRAIN_PATHS = os.path.join(OUTPUT_DIR, \"21\", \"train.parquet\")\n",
    "print(f\"Reading training data from: {TRAIN_PATHS}\")\n",
    "print(f\"Available days: {sorted([d for d in os.listdir(OUTPUT_DIR) if os.path.isdir(os.path.join(OUTPUT_DIR, d))])}\")\n",
    "print(f\"📁 Using classifier-specific directory: {OUTPUT_DIR}\")\n",
    "print(f\"🎯 Target format: Single value for binary classification (converts_for_a_topup-last)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e6358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(TRAIN_PATHS)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2eb355",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for examples of positive conversions (converts_for_a_topup-max = 1)\n",
    "print(\"🔍 Checking for positive conversion examples...\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "\n",
    "if 'converts_for_a_topup-max' in df.columns:\n",
    "    target_col = 'converts_for_a_topup-max'\n",
    "    print(f\"\\n📊 Target variable distribution:\")\n",
    "    print(df[target_col].value_counts().sort_index())\n",
    "    \n",
    "    conversion_rate = df[target_col].mean()\n",
    "    print(f\"\\n📈 Overall conversion rate: {conversion_rate:.3f} ({conversion_rate*100:.1f}%)\")\n",
    "    \n",
    "    # Show examples of positive conversions\n",
    "    positive_examples = df[df[target_col] == 1]\n",
    "    if len(positive_examples) > 0:\n",
    "        print(f\"\\n✅ Found {len(positive_examples)} positive conversion examples\")\n",
    "        print(\"\\n🎯 Sample positive conversions (converts_for_a_topup-max = 1):\")\n",
    "        # print(positive_examples[['loan_id', 'day-first', target_col]].head())\n",
    "        print(positive_examples[['loan_id', target_col]].head())\n",
    "        \n",
    "        # Show a few complete examples\n",
    "        print(f\"\\n📋 Complete feature example for a positive conversion:\")\n",
    "        sample_positive = positive_examples.iloc[0]\n",
    "        for col in df.columns:\n",
    "            if col.endswith('-list') or col.endswith('-max'):\n",
    "                print(f\"  {col}: {sample_positive[col]}\")\n",
    "    else:\n",
    "        print(\"❌ No positive conversion examples found in this dataset\")\n",
    "        \n",
    "    # Show examples of negative conversions  \n",
    "    negative_examples = df[df[target_col] == 0]\n",
    "    print(f\"\\n🔄 Found {len(negative_examples)} negative conversion examples\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Target column 'converts_for_a_topup-max' not found!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Check if we have the old list format instead\n",
    "    if 'converts_for_a_topup-list' in df.columns:\n",
    "        print(\"⚠️  Found 'converts_for_a_topup-list' - you may need to re-run the ETL pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae6339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Balanced Dataset for Training\n",
    "print(\"🎯 Creating balanced dataset for improved binary classification training...\")\n",
    "\n",
    "# Configuration: Ratio of non-conversions to conversions (negative:positive)\n",
    "# 1:10 ratio means 10 non-conversions for every 1 conversion\n",
    "NEGATIVE_TO_POSITIVE_RATIO = 10  # ⚙️ Configurable for experimentation\n",
    "\n",
    "print(f\"📊 Target ratio: {NEGATIVE_TO_POSITIVE_RATIO}:1 (negative:positive)\")\n",
    "\n",
    "# Load the processed data\n",
    "processed_data_path = os.path.join(INPUT_DATA_DIR, \"processed_nvt_classifier/part_0.parquet\")\n",
    "df_processed = pd.read_parquet(processed_data_path)\n",
    "\n",
    "print(f\"📈 Original dataset statistics:\")\n",
    "print(f\"   Total samples: {len(df_processed):,}\")\n",
    "\n",
    "# Check target distribution\n",
    "target_col = 'converts_for_a_topup-max'\n",
    "if target_col in df_processed.columns:\n",
    "    # Handle list format if needed\n",
    "    if hasattr(df_processed[target_col].iloc[0], '__len__') and not isinstance(df_processed[target_col].iloc[0], str):\n",
    "        print(\"🔧 Converting list format target to single values...\")\n",
    "        df_processed[target_col] = df_processed[target_col].apply(lambda x: x[0] if len(x) > 0 else 0)\n",
    "    \n",
    "    # Get original distribution\n",
    "    positive_samples = df_processed[df_processed[target_col] == 1]\n",
    "    negative_samples = df_processed[df_processed[target_col] == 0]\n",
    "    \n",
    "    original_positive = len(positive_samples)\n",
    "    original_negative = len(negative_samples)\n",
    "    original_ratio = original_negative / original_positive if original_positive > 0 else float('inf')\n",
    "    \n",
    "    print(f\"   Positive samples: {original_positive:,} ({original_positive/len(df_processed)*100:.2f}%)\")\n",
    "    print(f\"   Negative samples: {original_negative:,} ({original_negative/len(df_processed)*100:.2f}%)\")\n",
    "    print(f\"   Original ratio: {original_ratio:.1f}:1\")\n",
    "    \n",
    "    # Create balanced dataset\n",
    "    if original_positive > 0:\n",
    "        # Calculate target negative samples\n",
    "        target_negative_samples = original_positive * NEGATIVE_TO_POSITIVE_RATIO\n",
    "        \n",
    "        if target_negative_samples >= original_negative:\n",
    "            print(f\"⚠️  Warning: Target ratio ({NEGATIVE_TO_POSITIVE_RATIO}:1) requires {target_negative_samples:,} negative samples\")\n",
    "            print(f\"   but only {original_negative:,} available. Using all negative samples.\")\n",
    "            balanced_negative_samples = negative_samples\n",
    "        else:\n",
    "            # Randomly sample negative examples\n",
    "            print(f\"🎲 Randomly sampling {target_negative_samples:,} negative samples from {original_negative:,} available\")\n",
    "            balanced_negative_samples = negative_samples.sample(n=int(target_negative_samples), random_state=42)\n",
    "        \n",
    "        # Combine positive and balanced negative samples\n",
    "        balanced_df = pd.concat([positive_samples, balanced_negative_samples], ignore_index=True)\n",
    "        \n",
    "        # Shuffle the balanced dataset\n",
    "        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        # Final statistics\n",
    "        final_positive = len(balanced_df[balanced_df[target_col] == 1])\n",
    "        final_negative = len(balanced_df[balanced_df[target_col] == 0])\n",
    "        final_ratio = final_negative / final_positive if final_positive > 0 else float('inf')\n",
    "        \n",
    "        print(f\"\\n📊 Balanced dataset statistics:\")\n",
    "        print(f\"   Total samples: {len(balanced_df):,}\")\n",
    "        print(f\"   Positive samples: {final_positive:,} ({final_positive/len(balanced_df)*100:.2f}%)\")\n",
    "        print(f\"   Negative samples: {final_negative:,} ({final_negative/len(balanced_df)*100:.2f}%)\")\n",
    "        print(f\"   Final ratio: {final_ratio:.1f}:1\")\n",
    "        print(f\"   Size reduction: {(1 - len(balanced_df)/len(df_processed))*100:.1f}%\")\n",
    "        \n",
    "        # Save balanced dataset\n",
    "        balanced_output_dir = os.path.join(INPUT_DATA_DIR, f\"balanced_classifier_ratio_{NEGATIVE_TO_POSITIVE_RATIO}_to_1\")\n",
    "        os.makedirs(balanced_output_dir, exist_ok=True)\n",
    "        \n",
    "        balanced_file_path = os.path.join(balanced_output_dir, \"balanced_data.parquet\")\n",
    "        balanced_df.to_parquet(balanced_file_path, index=False)\n",
    "        \n",
    "        print(f\"\\n✅ Balanced dataset saved to: {balanced_file_path}\")\n",
    "        \n",
    "        # Save configuration metadata\n",
    "        config_info = {\n",
    "            'negative_to_positive_ratio': NEGATIVE_TO_POSITIVE_RATIO,\n",
    "            'original_samples': len(df_processed),\n",
    "            'original_positive': original_positive, \n",
    "            'original_negative': original_negative,\n",
    "            'original_ratio': original_ratio,\n",
    "            'balanced_samples': len(balanced_df),\n",
    "            'balanced_positive': final_positive,\n",
    "            'balanced_negative': final_negative,\n",
    "            'balanced_ratio': final_ratio,\n",
    "            'random_seed': 42\n",
    "        }\n",
    "        \n",
    "        import json\n",
    "        config_file_path = os.path.join(balanced_output_dir, \"balance_config.json\")\n",
    "        with open(config_file_path, 'w') as f:\n",
    "            json.dump(config_info, f, indent=2)\n",
    "            \n",
    "        print(f\"📄 Configuration saved to: {config_file_path}\")\n",
    "        \n",
    "        # Create stratified train/validation/test splits from balanced data\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        print(f\"\\n🔄 Creating stratified train/validation/test splits...\")\n",
    "        \n",
    "        # 70% train, 15% validation, 15% test\n",
    "        train_data, temp_data = train_test_split(\n",
    "            balanced_df, \n",
    "            test_size=0.3, \n",
    "            random_state=42, \n",
    "            stratify=balanced_df[target_col]\n",
    "        )\n",
    "        \n",
    "        val_data, test_data = train_test_split(\n",
    "            temp_data,\n",
    "            test_size=0.5,  # 0.5 of 0.3 = 0.15 (15% of total)\n",
    "            random_state=42,\n",
    "            stratify=temp_data[target_col]\n",
    "        )\n",
    "        \n",
    "        # Save splits\n",
    "        train_data.to_parquet(os.path.join(balanced_output_dir, \"train.parquet\"), index=False)\n",
    "        val_data.to_parquet(os.path.join(balanced_output_dir, \"valid.parquet\"), index=False)\n",
    "        test_data.to_parquet(os.path.join(balanced_output_dir, \"test.parquet\"), index=False)\n",
    "        \n",
    "        # Print split statistics\n",
    "        for split_name, split_data in [(\"TRAIN\", train_data), (\"VALIDATION\", val_data), (\"TEST\", test_data)]:\n",
    "            pos_count = (split_data[target_col] == 1).sum()\n",
    "            total_count = len(split_data)\n",
    "            conv_rate = pos_count / total_count if total_count > 0 else 0\n",
    "            \n",
    "            print(f\"   {split_name}:\")\n",
    "            print(f\"     Total: {total_count:,}\")\n",
    "            print(f\"     Positive: {pos_count:,} ({conv_rate:.1%})\")\n",
    "            print(f\"     Negative: {total_count - pos_count:,}\")\n",
    "        \n",
    "        print(f\"\\n🎯 Ready for training! Use balanced dataset at: {balanced_output_dir}\")\n",
    "        print(f\"💡 To experiment with different ratios, change NEGATIVE_TO_POSITIVE_RATIO and re-run this cell\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No positive samples found - cannot create balanced dataset\")\n",
    "        \n",
    "else:\n",
    "    print(f\"❌ Target column '{target_col}' not found!\")\n",
    "    print(f\"Available columns: {list(df_processed.columns)}\")\n",
    "\n",
    "# Clean up memory\n",
    "del df_processed\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687f998-8905-42a4-bb92-d1f5244860b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6461a96",
   "metadata": {},
   "source": [
    "You have  just created session-level features to train a session-based recommendation model using NVTabular. Now you can move to the the next notebook,`02-session-based-XLNet-with-PyT.ipynb` to train a session-based recommendation model using [XLNet](https://arxiv.org/abs/1906.08237), one of the state-of-the-art NLP model. Please shut down this kernel to free the GPU memory before you start the next one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d795d7ca5d3ec3bd6293cc80853205a74ce23d484a2b8f537732a716747107c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
