{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7783917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ======================================================================\n",
    "\n",
    "# Each user is responsible for checking the content of datasets and the\n",
    "# applicable licenses and determining if suitable for the intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6b360",
   "metadata": {},
   "source": [
    "<img src=\"https://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_transformers4rec_getting-started-session-based-01-etl-with-nvtabular/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# ETL with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6085c0",
   "metadata": {},
   "source": [
    "In this notebook we are going to generate synthetic data and then create sequential features with [NVTabular](https://github.com/NVIDIA-Merlin/NVTabular). Such data will be used in the next notebook to train a session-based recommendation model.\n",
    "\n",
    "NVTabular is a feature engineering and preprocessing library for tabular data designed to quickly and easily manipulate terabyte scale datasets used to train deep learning based recommender systems. It provides a high level abstraction to simplify code and accelerates computation on the GPU using the RAPIDS cuDF library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460a2bbe-6fa6-429f-89a8-7ab8b7cca299",
   "metadata": {},
   "source": [
    "<img src=\"https://nvidia-merlin.github.io/Transformers4Rec/main/_images/pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add26d16",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8dae24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'\n",
      "  warn(f\"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import glob\n",
    "\n",
    "import cudf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nvtabular as nvt\n",
    "from nvtabular.ops import *\n",
    "from merlin.schema.tags import Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3206b3f",
   "metadata": {},
   "source": [
    "### Define Input/Output Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "105dd71c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = os.environ.get(\"INPUT_DATA_DIR\", \"/workspace/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36498a01",
   "metadata": {},
   "source": [
    "## Load FSI Synthetic Demo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929036ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define paths to the FSI synthetic demo data files\n",
    "DATA_FILES = [\n",
    "    os.path.join(INPUT_DATA_DIR, \"synthetic_fsi\", \"Synthetic_Demo_Data_Shared_Part1.xlsx\"),\n",
    "    os.path.join(INPUT_DATA_DIR, \"synthetic_fsi\", \"Synthetic_Demo_Data_Shared_Part2.xlsx\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c65dfd-0cf7-4a5b-9383-4c10b50fd136",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from parquet file: /workspace/data/synthetic_fsi/synthetic_demo_data.parquet\n",
      "Loaded 440787 rows\n",
      "Total rows: 440787\n",
      "Available columns: ['session_date', 'loan_id', 'has_mobile_app', 'debtiq_enrolled', 'pa_eligible', 'topup_eligible', 'ita_eligible', 'email_sent_in_last_90_days', 'dm_sent_in_last_90_days', 'fico', 'income_', 'existing_loan_size_', 'current_loan_mob', 'offer___carousel', 'servicing___carousel', 'feature_sheet', 'bottom_sheet', 'converts_for_a_topup']\n"
     ]
    }
   ],
   "source": [
    "# Check if parquet file exists\n",
    "parquet_path = os.path.join(INPUT_DATA_DIR, \"synthetic_fsi\", \"synthetic_demo_data.parquet\")\n",
    "\n",
    "if os.path.exists(parquet_path):\n",
    "    print(f\"Loading data from parquet file: {parquet_path}\")\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "    print(f\"Loaded {len(df)} rows\")\n",
    "\n",
    "else:\n",
    "    print(\"Parquet file not found. Loading from Excel files...\")\n",
    "    dfs = []\n",
    "    for file_path in DATA_FILES:\n",
    "        print(f\"Reading {file_path}...\")\n",
    "        sheet_name = \"Data_Part1\" if \"Part1\" in file_path else \"Data_Part2\"\n",
    "        df_temp = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=1)  # Read from appropriate sheet\n",
    "        # Take first 100 rows for testing\n",
    "        # df_temp = df_temp.head(100)\n",
    "        # Clean column names to be more pythonic\n",
    "        df_temp.columns = [col.strip().lower()\n",
    "                          .replace(' ', '_')\n",
    "                          .replace('(', '')\n",
    "                          .replace(')', '')\n",
    "                          .replace('$', '')\n",
    "                          .replace('-', '_')\n",
    "                          for col in df_temp.columns]\n",
    "        dfs.append(df_temp)\n",
    "        print(f\"Loaded {len(df_temp)} rows from {os.path.basename(file_path)}\")\n",
    "\n",
    "    # Combine all dataframes\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Save to parquet for faster loading next time\n",
    "    print(f\"Saving data to parquet file: {parquet_path}\")\n",
    "    os.makedirs(os.path.dirname(parquet_path), exist_ok=True)\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Available columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705d0890-dabb-4821-bd39-101141669935",
   "metadata": {},
   "source": [
    "Visualize couple of rows of the loaded FSI synthetic demo dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcc4d884-eab6-4e7c-8570-61d24d066710",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_date</th>\n",
       "      <th>loan_id</th>\n",
       "      <th>has_mobile_app</th>\n",
       "      <th>debtiq_enrolled</th>\n",
       "      <th>pa_eligible</th>\n",
       "      <th>topup_eligible</th>\n",
       "      <th>ita_eligible</th>\n",
       "      <th>email_sent_in_last_90_days</th>\n",
       "      <th>dm_sent_in_last_90_days</th>\n",
       "      <th>fico</th>\n",
       "      <th>income_</th>\n",
       "      <th>existing_loan_size_</th>\n",
       "      <th>current_loan_mob</th>\n",
       "      <th>offer___carousel</th>\n",
       "      <th>servicing___carousel</th>\n",
       "      <th>feature_sheet</th>\n",
       "      <th>bottom_sheet</th>\n",
       "      <th>converts_for_a_topup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>4954838</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>807</td>\n",
       "      <td>57422</td>\n",
       "      <td>10857</td>\n",
       "      <td>9</td>\n",
       "      <td>Topup</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>4765835</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>741</td>\n",
       "      <td>63181</td>\n",
       "      <td>9287</td>\n",
       "      <td>28</td>\n",
       "      <td>Topup</td>\n",
       "      <td>Topup</td>\n",
       "      <td>blank</td>\n",
       "      <td>ITA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>4185554</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>745</td>\n",
       "      <td>50730</td>\n",
       "      <td>9720</td>\n",
       "      <td>28</td>\n",
       "      <td>Topup</td>\n",
       "      <td>Topup</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>7019817</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>741</td>\n",
       "      <td>148623</td>\n",
       "      <td>10786</td>\n",
       "      <td>12</td>\n",
       "      <td>Topup</td>\n",
       "      <td>ITA</td>\n",
       "      <td>blank</td>\n",
       "      <td>ITA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-24</td>\n",
       "      <td>8168610</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>752</td>\n",
       "      <td>110899</td>\n",
       "      <td>11537</td>\n",
       "      <td>27</td>\n",
       "      <td>Topup</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITA</td>\n",
       "      <td>ITA</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  session_date  loan_id  has_mobile_app  debtiq_enrolled  pa_eligible  \\\n",
       "0   2025-05-22  4954838               1                1            1   \n",
       "1   2025-05-21  4765835               1                1            1   \n",
       "2   2025-05-21  4185554               0                0            1   \n",
       "3   2025-05-21  7019817               0                0            1   \n",
       "4   2025-05-24  8168610               0                0            1   \n",
       "\n",
       "   topup_eligible  ita_eligible  email_sent_in_last_90_days  \\\n",
       "0               1             1                          17   \n",
       "1               1             1                          24   \n",
       "2               1             1                          20   \n",
       "3               1             1                          17   \n",
       "4               1             1                          19   \n",
       "\n",
       "   dm_sent_in_last_90_days  fico  income_  existing_loan_size_  \\\n",
       "0                        2   807    57422                10857   \n",
       "1                        1   741    63181                 9287   \n",
       "2                        2   745    50730                 9720   \n",
       "3                        1   741   148623                10786   \n",
       "4                        1   752   110899                11537   \n",
       "\n",
       "   current_loan_mob offer___carousel servicing___carousel feature_sheet  \\\n",
       "0                 9            Topup                  ITA           ITA   \n",
       "1                28            Topup                Topup         blank   \n",
       "2                28            Topup                Topup           ITA   \n",
       "3                12            Topup                  ITA         blank   \n",
       "4                27            Topup                  ITA           ITA   \n",
       "\n",
       "  bottom_sheet  converts_for_a_topup  \n",
       "0          ITA                     0  \n",
       "1          ITA                     0  \n",
       "2          ITA                     0  \n",
       "3          ITA                     0  \n",
       "4          ITA                     0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae36e04",
   "metadata": {},
   "source": [
    "## Feature Engineering with NVTabular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139de226",
   "metadata": {},
   "source": [
    "Deep Learning models require dense input features. Categorical features are sparse, and need to be represented by dense embeddings in the model. To allow for that, categorical features first need to be encoded as contiguous integers `(0, ..., |C|)`, where `|C|` is the feature cardinality (number of unique values), so that their embeddings can be efficiently stored in embedding layers.  We will use NVTabular to preprocess the categorical features, so that all categorical columns are encoded as contiguous integers. Note that the `Categorify` op encodes `nulls` to `1`, OOVs to `2` automatically. We preserve `0` for padding. The encoding of other categories starts from `3`. In our FSI demo dataset we handle any nulls appropriately. On the other hand `0` is used for padding the sequences in input block. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3bb9c",
   "metadata": {},
   "source": [
    "Here our goal is to create sequential features. To do so, we are grouping the features together at the session level in the following cell. In this FSI demo dataset, we may not have a timestamp column, but if we had one (that's the case for most real-world datasets), we would be sorting the interactions by the timestamp column as in this [example notebook](../end-to-end-session-based/01-ETL-with-NVTabular.ipynb). Note that we also trim each feature sequence in a  session to a certain length. Here, we use the NVTabular library so that we can easily preprocess and create features on GPU with a few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b7d9da9-6b0f-4295-8f77-6191a6a966cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing FSI data for sequential recommendation modeling...\n",
      "Unique product interactions: 7\n",
      "Applying NVTabular workflow to create sequential features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/merlin/schema/tags.py:149: UserWarning: Compound tags like Tags.ITEM_ID have been deprecated and will be removed in a future version. Please use the atomic versions of these tags, like [<Tags.ITEM: 'item'>, <Tags.ID: 'id'>].\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sequential feature engineering completed!\n",
      "üìä Features created for 440787 loan interactions\n",
      "üéØ Target variable: converts_for_a_topup (max value for binary classification)\n",
      "üìù Main item sequence: product_interaction (combinations of offers/services)\n",
      "üîÑ Note: This version is optimized for BinaryClassificationTask with single target values\n"
     ]
    }
   ],
   "source": [
    "# First, let's prepare the FSI data for sequential modeling\n",
    "print(\"Preparing FSI data for sequential recommendation modeling...\")\n",
    "\n",
    "# Convert session_date to datetime and extract day number for temporal features\n",
    "df['session_date'] = pd.to_datetime(df['session_date'])\n",
    "df['day'] = (df['session_date'] - df['session_date'].min()).dt.days + 1\n",
    "\n",
    "# Create a product interaction sequence - combining the carousel and sheet features\n",
    "# This represents the sequence of financial products/offers shown to each loan customer\n",
    "df['product_interaction'] = df['offer___carousel'].astype(str) + '_' + df['servicing___carousel'].astype(str)\n",
    "\n",
    "# For this FSI use case, we'll treat each unique product interaction as an \"item\"\n",
    "# and create sequences of these interactions per loan_id\n",
    "print(f\"Unique product interactions: {df['product_interaction'].nunique()}\")\n",
    "\n",
    "SESSIONS_MAX_LENGTH = 10  # Reduced for financial data which typically has shorter sequences\n",
    "\n",
    "# Define categorical features to encode\n",
    "categorical_features = [\n",
    "    'product_interaction',  # Our main \"item\" - combinations of offers/services\n",
    "    'offer___carousel',     # Individual offer type\n",
    "    'servicing___carousel', # Individual service type  \n",
    "    'feature_sheet',        # Feature sheet shown\n",
    "    'bottom_sheet'          # Bottom sheet type\n",
    "]\n",
    "\n",
    "# Define continuous/numerical features\n",
    "continuous_features = [\n",
    "    'fico',                        # Credit score\n",
    "    'income_',                     # Customer income\n",
    "    'existing_loan_size_',         # Current loan amount\n",
    "    'current_loan_mob',            # Months on book\n",
    "    'email_sent_in_last_90_days',  # Email frequency\n",
    "    'dm_sent_in_last_90_days'      # Direct mail frequency\n",
    "]\n",
    "\n",
    "# Define binary features (treat as categorical for embedding)\n",
    "binary_features = [\n",
    "    'has_mobile_app',\n",
    "    'debtiq_enrolled', \n",
    "    'pa_eligible',\n",
    "    'topup_eligible',\n",
    "    'ita_eligible'\n",
    "]\n",
    "\n",
    "# Categorify all categorical and binary features\n",
    "categ_feats = (categorical_features + binary_features) >> nvt.ops.Categorify()\n",
    "\n",
    "# Prepare continuous features \n",
    "cont_feats = continuous_features >> nvt.ops.FillMissing() >> nvt.ops.Normalize()\n",
    "\n",
    "# Define the complete feature set for groupby\n",
    "groupby_feats = categ_feats + cont_feats + ['loan_id', 'day', 'converts_for_a_topup']\n",
    "\n",
    "# Group features by loan_id to create sequences\n",
    "groupby_features = groupby_feats >> nvt.ops.Groupby(\n",
    "    groupby_cols=[\"loan_id\"], \n",
    "    aggs={\n",
    "        # Main product interaction sequence (our \"items\")\n",
    "        \"product_interaction\": [\"list\", \"count\"],\n",
    "        # Other categorical sequences\n",
    "        \"offer___carousel\": [\"list\"],\n",
    "        \"servicing___carousel\": [\"list\"], \n",
    "        \"feature_sheet\": [\"list\"],\n",
    "        \"bottom_sheet\": [\"list\"],\n",
    "        # Binary feature sequences\n",
    "        \"has_mobile_app\": [\"list\"],\n",
    "        \"debtiq_enrolled\": [\"list\"],\n",
    "        \"pa_eligible\": [\"list\"], \n",
    "        \"topup_eligible\": [\"list\"],\n",
    "        \"ita_eligible\": [\"list\"],\n",
    "        # Continuous feature sequences\n",
    "        \"fico\": [\"list\"],\n",
    "        \"income_\": [\"list\"],\n",
    "        \"existing_loan_size_\": [\"list\"],\n",
    "        \"current_loan_mob\": [\"list\"],\n",
    "        \"email_sent_in_last_90_days\": [\"list\"],\n",
    "        \"dm_sent_in_last_90_days\": [\"list\"],\n",
    "        # Temporal and target features\n",
    "        \"day\": [\"first\"],\n",
    "        \"converts_for_a_topup\": [\"max\"]  # Single target value for binary classification\n",
    "        },\n",
    "    name_sep=\"-\")\n",
    "\n",
    "# Create main item sequence (product interactions) with proper tagging\n",
    "sequence_features_item = (\n",
    "    groupby_features['product_interaction-list']\n",
    "    >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH) \n",
    "    >> TagAsItemID()\n",
    ")\n",
    "\n",
    "# Create categorical feature sequences\n",
    "categorical_sequences = (\n",
    "    groupby_features['offer___carousel-list', 'servicing___carousel-list', \n",
    "                    'feature_sheet-list', 'bottom_sheet-list'] \n",
    "    >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH)\n",
    ")\n",
    "\n",
    "# Create binary feature sequences \n",
    "binary_sequences = (\n",
    "    groupby_features['has_mobile_app-list', 'debtiq_enrolled-list', 'pa_eligible-list',\n",
    "                    'topup_eligible-list', 'ita_eligible-list']\n",
    "    >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH)\n",
    "    >> nvt.ops.AddMetadata(tags=[Tags.CATEGORICAL])\n",
    ")\n",
    "\n",
    "# Create continuous feature sequences\n",
    "continuous_sequences = (\n",
    "    groupby_features['fico-list', 'income_-list', 'existing_loan_size_-list',\n",
    "                    'current_loan_mob-list', 'email_sent_in_last_90_days-list',\n",
    "                    'dm_sent_in_last_90_days-list'] \n",
    "    >> nvt.ops.ListSlice(-SESSIONS_MAX_LENGTH)\n",
    "    >> nvt.ops.AddMetadata(tags=[Tags.CONTINUOUS])\n",
    ")\n",
    "\n",
    "# Create target feature (single value for binary classification)\n",
    "target_feature = (\n",
    "    groupby_features['converts_for_a_topup-max']\n",
    "    >> nvt.ops.AddMetadata(tags=[Tags.TARGET])\n",
    ")\n",
    "\n",
    "# Filter out loans with very short interaction sequences\n",
    "MINIMUM_SESSION_LENGTH = 1  # Keep all loans since FSI data may have single interactions\n",
    "selected_features = (\n",
    "    groupby_features['product_interaction-count', 'day-first', 'loan_id'] + \n",
    "    sequence_features_item +\n",
    "    categorical_sequences + \n",
    "    binary_sequences +\n",
    "    continuous_sequences +\n",
    "    target_feature\n",
    ")\n",
    "\n",
    "# For FSI data, we'll keep all loans even with single interactions    \n",
    "filtered_sessions = selected_features\n",
    "\n",
    "# Create final feature list with value counts for embedding dimensions\n",
    "seq_feats_list = filtered_sessions[\n",
    "    'product_interaction-list', 'offer___carousel-list', 'servicing___carousel-list',\n",
    "    'feature_sheet-list', 'bottom_sheet-list', 'has_mobile_app-list', \n",
    "    'debtiq_enrolled-list', 'pa_eligible-list', 'topup_eligible-list', \n",
    "    'ita_eligible-list', 'fico-list', 'income_-list', 'existing_loan_size_-list',\n",
    "    'current_loan_mob-list', 'email_sent_in_last_90_days-list', \n",
    "    'dm_sent_in_last_90_days-list', 'converts_for_a_topup-max'\n",
    "] >> nvt.ops.ValueCount()\n",
    "\n",
    "# Create the complete workflow\n",
    "workflow = nvt.Workflow(filtered_sessions['loan_id', 'day-first'] + seq_feats_list)\n",
    "\n",
    "# Apply the workflow to create sequential features\n",
    "print(\"Applying NVTabular workflow to create sequential features...\")\n",
    "dataset = nvt.Dataset(df)\n",
    "\n",
    "# Generate statistics for the features and export parquet files\n",
    "workflow.fit_transform(dataset).to_parquet(os.path.join(INPUT_DATA_DIR, \"processed_nvt_classifier\"))\n",
    "\n",
    "print(\"‚úÖ Sequential feature engineering completed!\")\n",
    "print(f\"üìä Features created for {len(df)} loan interactions\")\n",
    "print(f\"üéØ Target variable: converts_for_a_topup (max value for binary classification)\")\n",
    "print(f\"üìù Main item sequence: product_interaction (combinations of offers/services)\")\n",
    "print(f\"üîÑ Note: This version is optimized for BinaryClassificationTask with single target values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458c28f",
   "metadata": {},
   "source": [
    "It is possible to save the preprocessing workflow. That is useful to apply the same preprocessing to other data (with the same schema) and also to deploy the session-based recommendation pipeline to Triton Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e42cbf-edd6-44af-af23-c026edb578c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>tags</th>\n",
       "      <th>dtype</th>\n",
       "      <th>is_list</th>\n",
       "      <th>is_ragged</th>\n",
       "      <th>properties.num_buckets</th>\n",
       "      <th>properties.freq_threshold</th>\n",
       "      <th>properties.max_size</th>\n",
       "      <th>properties.start_index</th>\n",
       "      <th>properties.cat_path</th>\n",
       "      <th>properties.domain.min</th>\n",
       "      <th>properties.domain.max</th>\n",
       "      <th>properties.domain.name</th>\n",
       "      <th>properties.embedding_sizes.cardinality</th>\n",
       "      <th>properties.embedding_sizes.dimension</th>\n",
       "      <th>properties.value_count.min</th>\n",
       "      <th>properties.value_count.max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loan_id</td>\n",
       "      <td>()</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>day-first</td>\n",
       "      <td>()</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>product_interaction-list</td>\n",
       "      <td>(Tags.ID, Tags.ITEM_ID, Tags.LIST, Tags.CATEGO...</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.product_interaction.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>product_interaction</td>\n",
       "      <td>8.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>offer___carousel-list</td>\n",
       "      <td>(Tags.LIST, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.offer___carousel.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>offer___carousel</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>servicing___carousel-list</td>\n",
       "      <td>(Tags.LIST, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.servicing___carousel.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>servicing___carousel</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>feature_sheet-list</td>\n",
       "      <td>(Tags.LIST, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.feature_sheet.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>feature_sheet</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bottom_sheet-list</td>\n",
       "      <td>(Tags.LIST, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.bottom_sheet.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>bottom_sheet</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>has_mobile_app-list</td>\n",
       "      <td>(Tags.LIST, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.has_mobile_app.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>has_mobile_app</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>debtiq_enrolled-list</td>\n",
       "      <td>(Tags.LIST, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.debtiq_enrolled.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>debtiq_enrolled</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pa_eligible-list</td>\n",
       "      <td>(Tags.LIST, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.pa_eligible.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>pa_eligible</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>topup_eligible-list</td>\n",
       "      <td>(Tags.LIST, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.topup_eligible.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>topup_eligible</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ita_eligible-list</td>\n",
       "      <td>(Tags.LIST, Tags.CATEGORICAL)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>.//categories/unique.ita_eligible.parquet</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ita_eligible</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>fico-list</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.LIST)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>income_-list</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.LIST)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>existing_loan_size_-list</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.LIST)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>current_loan_mob-list</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.LIST)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>email_sent_in_last_90_days-list</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.LIST)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dm_sent_in_last_90_days-list</td>\n",
       "      <td>(Tags.CONTINUOUS, Tags.LIST)</td>\n",
       "      <td>DType(name='float64', element_type=&lt;ElementTyp...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>converts_for_a_topup-max</td>\n",
       "      <td>(Tags.TARGET)</td>\n",
       "      <td>DType(name='int64', element_type=&lt;ElementType....</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[{'name': 'loan_id', 'tags': set(), 'properties': {}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'day-first', 'tags': set(), 'properties': {}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None),))), 'is_list': False, 'is_ragged': False}, {'name': 'product_interaction-list', 'tags': {<Tags.ID: 'id'>, <Tags.ITEM_ID: 'item_id'>, <Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>, <Tags.ITEM: 'item'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.product_interaction.parquet', 'domain': {'min': 0, 'max': 7, 'name': 'product_interaction'}, 'embedding_sizes': {'cardinality': 8, 'dimension': 16}, 'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'offer___carousel-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.offer___carousel.parquet', 'domain': {'min': 0, 'max': 4, 'name': 'offer___carousel'}, 'embedding_sizes': {'cardinality': 5, 'dimension': 16}, 'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'servicing___carousel-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.servicing___carousel.parquet', 'domain': {'min': 0, 'max': 4, 'name': 'servicing___carousel'}, 'embedding_sizes': {'cardinality': 5, 'dimension': 16}, 'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'feature_sheet-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.feature_sheet.parquet', 'domain': {'min': 0, 'max': 4, 'name': 'feature_sheet'}, 'embedding_sizes': {'cardinality': 5, 'dimension': 16}, 'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'bottom_sheet-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.bottom_sheet.parquet', 'domain': {'min': 0, 'max': 4, 'name': 'bottom_sheet'}, 'embedding_sizes': {'cardinality': 5, 'dimension': 16}, 'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'has_mobile_app-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.has_mobile_app.parquet', 'domain': {'min': 0, 'max': 2, 'name': 'has_mobile_app'}, 'embedding_sizes': {'cardinality': 3, 'dimension': 16}, 'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'debtiq_enrolled-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.debtiq_enrolled.parquet', 'domain': {'min': 0, 'max': 2, 'name': 'debtiq_enrolled'}, 'embedding_sizes': {'cardinality': 3, 'dimension': 16}, 'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'pa_eligible-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.pa_eligible.parquet', 'domain': {'min': 0, 'max': 2, 'name': 'pa_eligible'}, 'embedding_sizes': {'cardinality': 3, 'dimension': 16}, 'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'topup_eligible-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.topup_eligible.parquet', 'domain': {'min': 0, 'max': 2, 'name': 'topup_eligible'}, 'embedding_sizes': {'cardinality': 3, 'dimension': 16}, 'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'ita_eligible-list', 'tags': {<Tags.LIST: 'list'>, <Tags.CATEGORICAL: 'categorical'>}, 'properties': {'num_buckets': None, 'freq_threshold': 0, 'max_size': 0, 'start_index': 0, 'cat_path': './/categories/unique.ita_eligible.parquet', 'domain': {'min': 0, 'max': 1, 'name': 'ita_eligible'}, 'embedding_sizes': {'cardinality': 2, 'dimension': 16}, 'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'fico-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.LIST: 'list'>}, 'properties': {'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'income_-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.LIST: 'list'>}, 'properties': {'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'existing_loan_size_-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.LIST: 'list'>}, 'properties': {'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'current_loan_mob-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.LIST: 'list'>}, 'properties': {'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'email_sent_in_last_90_days-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.LIST: 'list'>}, 'properties': {'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'dm_sent_in_last_90_days-list', 'tags': {<Tags.CONTINUOUS: 'continuous'>, <Tags.LIST: 'list'>}, 'properties': {'value_count': {'min': 1, 'max': 9}}, 'dtype': DType(name='float64', element_type=<ElementType.Float: 'float'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=1, max=9)))), 'is_list': True, 'is_ragged': True}, {'name': 'converts_for_a_topup-max', 'tags': {<Tags.TARGET: 'target'>}, 'properties': {'value_count': {'min': 0, 'max': None}}, 'dtype': DType(name='int64', element_type=<ElementType.Int: 'int'>, element_size=64, element_unit=None, signed=True, shape=Shape(dims=(Dimension(min=0, max=None), Dimension(min=0, max=None)))), 'is_list': True, 'is_ragged': True}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow.output_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b54bb-4549-49a3-89bb-1f573a426aca",
   "metadata": {},
   "source": [
    "Save NVTabular workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f498dce-69eb-4f88-8ddd-8629558825df",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.save(os.path.join(INPUT_DATA_DIR, \"workflow_etl_classifier\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a41961",
   "metadata": {},
   "source": [
    "## Export pre-processed data by day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cedca3",
   "metadata": {},
   "source": [
    "In this example we are going to split the preprocessed parquet files by days, to allow for temporal training and evaluation. There will be a folder for each day and three parquet files within each day folder: `train.parquet`, `validation.parquet` and `test.parquet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d3e59b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.environ.get(\"OUTPUT_DIR\",os.path.join(INPUT_DATA_DIR, \"sessions_by_day_classifier\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "603fb27a-0c64-43eb-be79-42213944990b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the processed parquet file\n",
    "sessions_gdf = cudf.read_parquet(os.path.join(INPUT_DATA_DIR, \"processed_nvt_classifier/part_0.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c537a248-059e-4db9-8b62-9681175f0193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   loan_id  day-first product_interaction-list offer___carousel-list  \\\n",
      "0  1458465         24                [2, 2, 2]             [2, 2, 2]   \n",
      "1  1458476         26                [1, 1, 1]             [1, 1, 1]   \n",
      "2  1458488         30                [1, 1, 1]             [1, 1, 1]   \n",
      "\n",
      "  servicing___carousel-list feature_sheet-list bottom_sheet-list  \\\n",
      "0                 [2, 2, 2]          [3, 3, 3]         [4, 4, 4]   \n",
      "1                 [1, 1, 1]          [1, 1, 1]         [1, 1, 1]   \n",
      "2                 [1, 1, 1]          [1, 1, 1]         [1, 1, 1]   \n",
      "\n",
      "  has_mobile_app-list debtiq_enrolled-list pa_eligible-list  \\\n",
      "0           [2, 2, 2]            [1, 1, 1]        [2, 2, 2]   \n",
      "1           [1, 1, 1]            [2, 2, 2]        [1, 1, 1]   \n",
      "2           [1, 1, 1]            [2, 2, 2]        [1, 1, 1]   \n",
      "\n",
      "  topup_eligible-list ita_eligible-list  \\\n",
      "0           [2, 2, 2]         [1, 1, 1]   \n",
      "1           [1, 1, 1]         [1, 1, 1]   \n",
      "2           [1, 1, 1]         [1, 1, 1]   \n",
      "\n",
      "                                           fico-list  \\\n",
      "0  [1.9899586062139822, 1.9899586062139822, 1.989...   \n",
      "1  [-0.8386216672353174, -0.8386216672353174, -0....   \n",
      "2  [-1.8036667017062549, -1.8036667017062549, -1....   \n",
      "\n",
      "                                        income_-list  \\\n",
      "0  [0.38238719824614986, 0.38238719824614986, 0.3...   \n",
      "1  [0.7092552805503524, 0.7092552805503524, 0.709...   \n",
      "2  [0.7940793777318331, 0.7940793777318331, 0.794...   \n",
      "\n",
      "                            existing_loan_size_-list  \\\n",
      "0  [0.05207997838482223, 0.05207997838482223, 0.0...   \n",
      "1  [1.2607242224762425, 1.2607242224762425, 1.260...   \n",
      "2  [-1.7349925591873965, -1.7349925591873965, -1....   \n",
      "\n",
      "                               current_loan_mob-list  \\\n",
      "0  [-0.003984065196468962, -0.003984065196468962,...   \n",
      "1  [-0.6272480360073204, -0.6272480360073204, -0....   \n",
      "2  [1.3204518727765902, 1.3204518727765902, 1.320...   \n",
      "\n",
      "                     email_sent_in_last_90_days-list  \\\n",
      "0  [-0.0024832308712834, -0.0024832308712834, -0....   \n",
      "1  [0.3851144624928664, 0.3851144624928664, 0.385...   \n",
      "2  [-1.5528740043278828, -1.5528740043278828, -1....   \n",
      "\n",
      "                        dm_sent_in_last_90_days-list  converts_for_a_topup-max  \n",
      "0  [-1.6188956751970283, -1.6188956751970283, -1....                         0  \n",
      "1  [1.3952755912957482, 1.3952755912957482, 1.395...                         0  \n",
      "2  [-0.4132271685999176, -0.4132271685999176, -0....                         0  \n"
     ]
    }
   ],
   "source": [
    "print(sessions_gdf.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c67a92b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating time-based splits: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:02<00:00,  4.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers4rec.utils.data_utils import save_time_based_splits\n",
    "save_time_based_splits(data=nvt.Dataset(sessions_gdf),\n",
    "                       output_dir= OUTPUT_DIR,\n",
    "                       partition_col='day-first',\n",
    "                       timestamp_col='loan_id', \n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72337b",
   "metadata": {},
   "source": [
    "## Check out the preprocessed outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd04ec82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from: /workspace/data/sessions_by_day_classifier/21/train.parquet\n",
      "Available days: ['21', '22', '23', '24', '25', '26', '27', '28', '29', '30']\n",
      "üìÅ Using classifier-specific directory: /workspace/data/sessions_by_day_classifier\n",
      "üéØ Target format: Single value for binary classification (converts_for_a_topup-last)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_PATHS = os.path.join(OUTPUT_DIR, \"1\", \"train.parquet\")\n",
    "TRAIN_PATHS = os.path.join(OUTPUT_DIR, \"21\", \"train.parquet\")\n",
    "print(f\"Reading training data from: {TRAIN_PATHS}\")\n",
    "print(f\"Available days: {sorted([d for d in os.listdir(OUTPUT_DIR) if os.path.isdir(os.path.join(OUTPUT_DIR, d))])}\")\n",
    "print(f\"üìÅ Using classifier-specific directory: {OUTPUT_DIR}\")\n",
    "print(f\"üéØ Target format: Single value for binary classification (converts_for_a_topup-last)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e5e6358",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_id</th>\n",
       "      <th>product_interaction-list</th>\n",
       "      <th>offer___carousel-list</th>\n",
       "      <th>servicing___carousel-list</th>\n",
       "      <th>feature_sheet-list</th>\n",
       "      <th>bottom_sheet-list</th>\n",
       "      <th>has_mobile_app-list</th>\n",
       "      <th>debtiq_enrolled-list</th>\n",
       "      <th>pa_eligible-list</th>\n",
       "      <th>topup_eligible-list</th>\n",
       "      <th>ita_eligible-list</th>\n",
       "      <th>fico-list</th>\n",
       "      <th>income_-list</th>\n",
       "      <th>existing_loan_size_-list</th>\n",
       "      <th>current_loan_mob-list</th>\n",
       "      <th>email_sent_in_last_90_days-list</th>\n",
       "      <th>dm_sent_in_last_90_days-list</th>\n",
       "      <th>converts_for_a_topup-max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1458616</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[3, 3, 3]</td>\n",
       "      <td>[4, 4, 4]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0.9417200342886536, 0.9417200342886536, 0.941...</td>\n",
       "      <td>[-1.0976320556943726, -1.0976320556943726, -1....</td>\n",
       "      <td>[0.57659599290483, 0.57659599290483, 0.5765959...</td>\n",
       "      <td>[-0.003984065196468962, -0.003984065196468962,...</td>\n",
       "      <td>[-0.777678617599583, -0.777678617599583, -0.77...</td>\n",
       "      <td>[-1.016061421898473, -1.016061421898473, -1.01...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1459233</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[-1.75375057923362, -1.75375057923362, -1.7537...</td>\n",
       "      <td>[-1.5410109814360171, -1.5410109814360171, -1....</td>\n",
       "      <td>[-1.4147058181691032, -1.4147058181691032, -1....</td>\n",
       "      <td>[-0.6272480360073204, -0.6272480360073204, -0....</td>\n",
       "      <td>[1.160309849221166, 1.160309849221166, 1.16030...</td>\n",
       "      <td>[1.3952755912957482, 1.3952755912957482, 1.395...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1460608</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[-0.5058475174177527, -0.5058475174177527, -0....</td>\n",
       "      <td>[1.5863748535841433, 1.5863748535841433, 1.586...</td>\n",
       "      <td>[0.7702021572517088, 0.7702021572517088, 0.770...</td>\n",
       "      <td>[-1.4063279995208846, -1.4063279995208846, -1....</td>\n",
       "      <td>[1.160309849221166, 1.160309849221166, 1.16030...</td>\n",
       "      <td>[0.18960708469863766, 0.18960708469863766, 0.1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1461617</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[4, 4, 4]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0.14306207472649837, 0.14306207472649837, 0.1...</td>\n",
       "      <td>[-0.4086219660085771, -0.4086219660085771, -0....</td>\n",
       "      <td>[1.0498555057527559, 1.0498555057527559, 1.049...</td>\n",
       "      <td>[0.30764792020895676, 0.30764792020895676, 0.3...</td>\n",
       "      <td>[1.547907542585316, 1.547907542585316, 1.54790...</td>\n",
       "      <td>[-0.4132271685999176, -0.4132271685999176, -0....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1462517</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[2, 2, 2]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[-1.8369441166880114, -1.8369441166880114, -1....</td>\n",
       "      <td>[0.8277590011108106, 0.8277590011108106, 0.827...</td>\n",
       "      <td>[1.3743914904329835, 1.3743914904329835, 1.374...</td>\n",
       "      <td>[0.6192799056143824, 0.6192799056143824, 0.619...</td>\n",
       "      <td>[-0.0024832308712834, -0.0024832308712834, -0....</td>\n",
       "      <td>[1.3952755912957482, 1.3952755912957482, 1.395...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_id product_interaction-list offer___carousel-list  \\\n",
       "0  1458616                [2, 2, 2]             [2, 2, 2]   \n",
       "1  1459233                [1, 1, 1]             [1, 1, 1]   \n",
       "2  1460608                [1, 1, 1]             [1, 1, 1]   \n",
       "4  1461617                [2, 2, 2]             [2, 2, 2]   \n",
       "5  1462517                [1, 1, 1]             [1, 1, 1]   \n",
       "\n",
       "  servicing___carousel-list feature_sheet-list bottom_sheet-list  \\\n",
       "0                 [2, 2, 2]          [3, 3, 3]         [4, 4, 4]   \n",
       "1                 [1, 1, 1]          [1, 1, 1]         [1, 1, 1]   \n",
       "2                 [1, 1, 1]          [1, 1, 1]         [1, 1, 1]   \n",
       "4                 [2, 2, 2]          [4, 4, 4]         [2, 2, 2]   \n",
       "5                 [1, 1, 1]          [1, 1, 1]         [1, 1, 1]   \n",
       "\n",
       "  has_mobile_app-list debtiq_enrolled-list pa_eligible-list  \\\n",
       "0           [1, 1, 1]            [1, 1, 1]        [2, 2, 2]   \n",
       "1           [1, 1, 1]            [2, 2, 2]        [1, 1, 1]   \n",
       "2           [2, 2, 2]            [2, 2, 2]        [1, 1, 1]   \n",
       "4           [1, 1, 1]            [2, 2, 2]        [1, 1, 1]   \n",
       "5           [1, 1, 1]            [2, 2, 2]        [1, 1, 1]   \n",
       "\n",
       "  topup_eligible-list ita_eligible-list  \\\n",
       "0           [2, 2, 2]         [1, 1, 1]   \n",
       "1           [1, 1, 1]         [1, 1, 1]   \n",
       "2           [1, 1, 1]         [1, 1, 1]   \n",
       "4           [2, 2, 2]         [1, 1, 1]   \n",
       "5           [1, 1, 1]         [1, 1, 1]   \n",
       "\n",
       "                                           fico-list  \\\n",
       "0  [0.9417200342886536, 0.9417200342886536, 0.941...   \n",
       "1  [-1.75375057923362, -1.75375057923362, -1.7537...   \n",
       "2  [-0.5058475174177527, -0.5058475174177527, -0....   \n",
       "4  [0.14306207472649837, 0.14306207472649837, 0.1...   \n",
       "5  [-1.8369441166880114, -1.8369441166880114, -1....   \n",
       "\n",
       "                                        income_-list  \\\n",
       "0  [-1.0976320556943726, -1.0976320556943726, -1....   \n",
       "1  [-1.5410109814360171, -1.5410109814360171, -1....   \n",
       "2  [1.5863748535841433, 1.5863748535841433, 1.586...   \n",
       "4  [-0.4086219660085771, -0.4086219660085771, -0....   \n",
       "5  [0.8277590011108106, 0.8277590011108106, 0.827...   \n",
       "\n",
       "                            existing_loan_size_-list  \\\n",
       "0  [0.57659599290483, 0.57659599290483, 0.5765959...   \n",
       "1  [-1.4147058181691032, -1.4147058181691032, -1....   \n",
       "2  [0.7702021572517088, 0.7702021572517088, 0.770...   \n",
       "4  [1.0498555057527559, 1.0498555057527559, 1.049...   \n",
       "5  [1.3743914904329835, 1.3743914904329835, 1.374...   \n",
       "\n",
       "                               current_loan_mob-list  \\\n",
       "0  [-0.003984065196468962, -0.003984065196468962,...   \n",
       "1  [-0.6272480360073204, -0.6272480360073204, -0....   \n",
       "2  [-1.4063279995208846, -1.4063279995208846, -1....   \n",
       "4  [0.30764792020895676, 0.30764792020895676, 0.3...   \n",
       "5  [0.6192799056143824, 0.6192799056143824, 0.619...   \n",
       "\n",
       "                     email_sent_in_last_90_days-list  \\\n",
       "0  [-0.777678617599583, -0.777678617599583, -0.77...   \n",
       "1  [1.160309849221166, 1.160309849221166, 1.16030...   \n",
       "2  [1.160309849221166, 1.160309849221166, 1.16030...   \n",
       "4  [1.547907542585316, 1.547907542585316, 1.54790...   \n",
       "5  [-0.0024832308712834, -0.0024832308712834, -0....   \n",
       "\n",
       "                        dm_sent_in_last_90_days-list  converts_for_a_topup-max  \n",
       "0  [-1.016061421898473, -1.016061421898473, -1.01...                         0  \n",
       "1  [1.3952755912957482, 1.3952755912957482, 1.395...                         0  \n",
       "2  [0.18960708469863766, 0.18960708469863766, 0.1...                         0  \n",
       "4  [-0.4132271685999176, -0.4132271685999176, -0....                         0  \n",
       "5  [1.3952755912957482, 1.3952755912957482, 1.395...                         0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(TRAIN_PATHS)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e2eb355",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking for positive conversion examples...\n",
      "Total records: 11780\n",
      "\n",
      "üìä Target variable distribution:\n",
      "0    11503\n",
      "1      277\n",
      "Name: converts_for_a_topup-max, dtype: int64\n",
      "\n",
      "üìà Overall conversion rate: 0.024 (2.4%)\n",
      "\n",
      "‚úÖ Found 277 positive conversion examples\n",
      "\n",
      "üéØ Sample positive conversions (converts_for_a_topup-max = 1):\n",
      "     loan_id  converts_for_a_topup-max\n",
      "57   1488467                         1\n",
      "58   1489156                         1\n",
      "149  1539967                         1\n",
      "161  1548600                         1\n",
      "195  1567374                         1\n",
      "\n",
      "üìã Complete feature example for a positive conversion:\n",
      "  product_interaction-list: [2 2 2]\n",
      "  offer___carousel-list: [2 2 2]\n",
      "  servicing___carousel-list: [2 2 2]\n",
      "  feature_sheet-list: [1 1 1]\n",
      "  bottom_sheet-list: [2 2 2]\n",
      "  has_mobile_app-list: [1 1 1]\n",
      "  debtiq_enrolled-list: [1 1 1]\n",
      "  pa_eligible-list: [1 1 1]\n",
      "  topup_eligible-list: [2 2 2]\n",
      "  ita_eligible-list: [1 1 1]\n",
      "  fico-list: [0.74205554 0.74205554 0.74205554]\n",
      "  income_-list: [1.17999097 1.17999097 1.17999097]\n",
      "  existing_loan_size_-list: [0.48629957 0.48629957 0.48629957]\n",
      "  current_loan_mob-list: [0.85300389 0.85300389 0.85300389]\n",
      "  email_sent_in_last_90_days-list: [-1.16527631 -1.16527631 -1.16527631]\n",
      "  dm_sent_in_last_90_days-list: [1.39527559 1.39527559 1.39527559]\n",
      "  converts_for_a_topup-max: 1\n",
      "\n",
      "üîÑ Found 11503 negative conversion examples\n"
     ]
    }
   ],
   "source": [
    "# Check for examples of positive conversions (converts_for_a_topup-max = 1)\n",
    "print(\"üîç Checking for positive conversion examples...\")\n",
    "print(f\"Total records: {len(df)}\")\n",
    "\n",
    "if 'converts_for_a_topup-max' in df.columns:\n",
    "    target_col = 'converts_for_a_topup-max'\n",
    "    print(f\"\\nüìä Target variable distribution:\")\n",
    "    print(df[target_col].value_counts().sort_index())\n",
    "    \n",
    "    conversion_rate = df[target_col].mean()\n",
    "    print(f\"\\nüìà Overall conversion rate: {conversion_rate:.3f} ({conversion_rate*100:.1f}%)\")\n",
    "    \n",
    "    # Show examples of positive conversions\n",
    "    positive_examples = df[df[target_col] == 1]\n",
    "    if len(positive_examples) > 0:\n",
    "        print(f\"\\n‚úÖ Found {len(positive_examples)} positive conversion examples\")\n",
    "        print(\"\\nüéØ Sample positive conversions (converts_for_a_topup-max = 1):\")\n",
    "        # print(positive_examples[['loan_id', 'day-first', target_col]].head())\n",
    "        print(positive_examples[['loan_id', target_col]].head())\n",
    "        \n",
    "        # Show a few complete examples\n",
    "        print(f\"\\nüìã Complete feature example for a positive conversion:\")\n",
    "        sample_positive = positive_examples.iloc[0]\n",
    "        for col in df.columns:\n",
    "            if col.endswith('-list') or col.endswith('-max'):\n",
    "                print(f\"  {col}: {sample_positive[col]}\")\n",
    "    else:\n",
    "        print(\"‚ùå No positive conversion examples found in this dataset\")\n",
    "        \n",
    "    # Show examples of negative conversions  \n",
    "    negative_examples = df[df[target_col] == 0]\n",
    "    print(f\"\\nüîÑ Found {len(negative_examples)} negative conversion examples\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Target column 'converts_for_a_topup-max' not found!\")\n",
    "    print(f\"Available columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Check if we have the old list format instead\n",
    "    if 'converts_for_a_topup-list' in df.columns:\n",
    "        print(\"‚ö†Ô∏è  Found 'converts_for_a_topup-list' - you may need to re-run the ETL pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aae6339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Creating balanced dataset for improved binary classification training...\n",
      "üìä Target ratio: 1:1 (negative:positive)\n",
      "üìà Original dataset statistics:\n",
      "   Total samples: 148,570\n",
      "   Positive samples: 3,419 (2.30%)\n",
      "   Negative samples: 145,151 (97.70%)\n",
      "   Original ratio: 42.5:1\n",
      "üé≤ Randomly sampling 3,419 negative samples from 145,151 available\n",
      "\n",
      "üìä Balanced dataset statistics:\n",
      "   Total samples: 6,838\n",
      "   Positive samples: 3,419 (50.00%)\n",
      "   Negative samples: 3,419 (50.00%)\n",
      "   Final ratio: 1.0:1\n",
      "   Size reduction: 95.4%\n",
      "\n",
      "‚úÖ Balanced dataset saved to: /workspace/data/balanced_classifier_ratio_1_to_1/balanced_data.parquet\n",
      "üìÑ Configuration saved to: /workspace/data/balanced_classifier_ratio_1_to_1/balance_config.json\n",
      "\n",
      "üîÑ Creating stratified train/validation/test splits...\n",
      "   TRAIN:\n",
      "     Total: 4,786\n",
      "     Positive: 2,393 (50.0%)\n",
      "     Negative: 2,393\n",
      "   VALIDATION:\n",
      "     Total: 1,026\n",
      "     Positive: 513 (50.0%)\n",
      "     Negative: 513\n",
      "   TEST:\n",
      "     Total: 1,026\n",
      "     Positive: 513 (50.0%)\n",
      "     Negative: 513\n",
      "\n",
      "üéØ Ready for training! Use balanced dataset at: /workspace/data/balanced_classifier_ratio_1_to_1\n",
      "üí° To experiment with different ratios, change NEGATIVE_TO_POSITIVE_RATIO and re-run this cell\n"
     ]
    }
   ],
   "source": [
    "# Create Balanced Dataset for Training\n",
    "print(\"üéØ Creating balanced dataset for improved binary classification training...\")\n",
    "\n",
    "# Configuration: Ratio of non-conversions to conversions (negative:positive)\n",
    "# 1:10 ratio means 10 non-conversions for every 1 conversion\n",
    "NEGATIVE_TO_POSITIVE_RATIO = 1 #10  # ‚öôÔ∏è Configurable for experimentation\n",
    "\n",
    "print(f\"üìä Target ratio: {NEGATIVE_TO_POSITIVE_RATIO}:1 (negative:positive)\")\n",
    "\n",
    "# Load the processed data\n",
    "processed_data_path = os.path.join(INPUT_DATA_DIR, \"processed_nvt_classifier/part_0.parquet\")\n",
    "df_processed = pd.read_parquet(processed_data_path)\n",
    "\n",
    "print(f\"üìà Original dataset statistics:\")\n",
    "print(f\"   Total samples: {len(df_processed):,}\")\n",
    "\n",
    "# Check target distribution\n",
    "target_col = 'converts_for_a_topup-max'\n",
    "if target_col in df_processed.columns:\n",
    "    # Handle list format if needed\n",
    "    if hasattr(df_processed[target_col].iloc[0], '__len__') and not isinstance(df_processed[target_col].iloc[0], str):\n",
    "        print(\"üîß Converting list format target to single values...\")\n",
    "        df_processed[target_col] = df_processed[target_col].apply(lambda x: x[0] if len(x) > 0 else 0)\n",
    "    \n",
    "    # Get original distribution\n",
    "    positive_samples = df_processed[df_processed[target_col] == 1]\n",
    "    negative_samples = df_processed[df_processed[target_col] == 0]\n",
    "    \n",
    "    original_positive = len(positive_samples)\n",
    "    original_negative = len(negative_samples)\n",
    "    original_ratio = original_negative / original_positive if original_positive > 0 else float('inf')\n",
    "    \n",
    "    print(f\"   Positive samples: {original_positive:,} ({original_positive/len(df_processed)*100:.2f}%)\")\n",
    "    print(f\"   Negative samples: {original_negative:,} ({original_negative/len(df_processed)*100:.2f}%)\")\n",
    "    print(f\"   Original ratio: {original_ratio:.1f}:1\")\n",
    "    \n",
    "    # Create balanced dataset\n",
    "    if original_positive > 0:\n",
    "        # Calculate target negative samples\n",
    "        target_negative_samples = original_positive * NEGATIVE_TO_POSITIVE_RATIO\n",
    "        \n",
    "        if target_negative_samples >= original_negative:\n",
    "            print(f\"‚ö†Ô∏è  Warning: Target ratio ({NEGATIVE_TO_POSITIVE_RATIO}:1) requires {target_negative_samples:,} negative samples\")\n",
    "            print(f\"   but only {original_negative:,} available. Using all negative samples.\")\n",
    "            balanced_negative_samples = negative_samples\n",
    "        else:\n",
    "            # Randomly sample negative examples\n",
    "            print(f\"üé≤ Randomly sampling {target_negative_samples:,} negative samples from {original_negative:,} available\")\n",
    "            balanced_negative_samples = negative_samples.sample(n=int(target_negative_samples), random_state=42)\n",
    "        \n",
    "        # Combine positive and balanced negative samples\n",
    "        balanced_df = pd.concat([positive_samples, balanced_negative_samples], ignore_index=True)\n",
    "        \n",
    "        # Shuffle the balanced dataset\n",
    "        balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        # Final statistics\n",
    "        final_positive = len(balanced_df[balanced_df[target_col] == 1])\n",
    "        final_negative = len(balanced_df[balanced_df[target_col] == 0])\n",
    "        final_ratio = final_negative / final_positive if final_positive > 0 else float('inf')\n",
    "        \n",
    "        print(f\"\\nüìä Balanced dataset statistics:\")\n",
    "        print(f\"   Total samples: {len(balanced_df):,}\")\n",
    "        print(f\"   Positive samples: {final_positive:,} ({final_positive/len(balanced_df)*100:.2f}%)\")\n",
    "        print(f\"   Negative samples: {final_negative:,} ({final_negative/len(balanced_df)*100:.2f}%)\")\n",
    "        print(f\"   Final ratio: {final_ratio:.1f}:1\")\n",
    "        print(f\"   Size reduction: {(1 - len(balanced_df)/len(df_processed))*100:.1f}%\")\n",
    "        \n",
    "        # Save balanced dataset\n",
    "        balanced_output_dir = os.path.join(INPUT_DATA_DIR, f\"balanced_classifier_ratio_{NEGATIVE_TO_POSITIVE_RATIO}_to_1\")\n",
    "        os.makedirs(balanced_output_dir, exist_ok=True)\n",
    "        \n",
    "        balanced_file_path = os.path.join(balanced_output_dir, \"balanced_data.parquet\")\n",
    "        balanced_df.to_parquet(balanced_file_path, index=False)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Balanced dataset saved to: {balanced_file_path}\")\n",
    "        \n",
    "        # Save configuration metadata\n",
    "        config_info = {\n",
    "            'negative_to_positive_ratio': NEGATIVE_TO_POSITIVE_RATIO,\n",
    "            'original_samples': len(df_processed),\n",
    "            'original_positive': original_positive, \n",
    "            'original_negative': original_negative,\n",
    "            'original_ratio': original_ratio,\n",
    "            'balanced_samples': len(balanced_df),\n",
    "            'balanced_positive': final_positive,\n",
    "            'balanced_negative': final_negative,\n",
    "            'balanced_ratio': final_ratio,\n",
    "            'random_seed': 42\n",
    "        }\n",
    "        \n",
    "        import json\n",
    "        config_file_path = os.path.join(balanced_output_dir, \"balance_config.json\")\n",
    "        with open(config_file_path, 'w') as f:\n",
    "            json.dump(config_info, f, indent=2)\n",
    "            \n",
    "        print(f\"üìÑ Configuration saved to: {config_file_path}\")\n",
    "        \n",
    "        # Create stratified train/validation/test splits from balanced data\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        print(f\"\\nüîÑ Creating stratified train/validation/test splits...\")\n",
    "        \n",
    "        # 70% train, 15% validation, 15% test\n",
    "        train_data, temp_data = train_test_split(\n",
    "            balanced_df, \n",
    "            test_size=0.3, \n",
    "            random_state=42, \n",
    "            stratify=balanced_df[target_col]\n",
    "        )\n",
    "        \n",
    "        val_data, test_data = train_test_split(\n",
    "            temp_data,\n",
    "            test_size=0.5,  # 0.5 of 0.3 = 0.15 (15% of total)\n",
    "            random_state=42,\n",
    "            stratify=temp_data[target_col]\n",
    "        )\n",
    "        \n",
    "        # Save splits\n",
    "        train_data.to_parquet(os.path.join(balanced_output_dir, \"train.parquet\"), index=False)\n",
    "        val_data.to_parquet(os.path.join(balanced_output_dir, \"valid.parquet\"), index=False)\n",
    "        test_data.to_parquet(os.path.join(balanced_output_dir, \"test.parquet\"), index=False)\n",
    "        \n",
    "        # Print split statistics\n",
    "        for split_name, split_data in [(\"TRAIN\", train_data), (\"VALIDATION\", val_data), (\"TEST\", test_data)]:\n",
    "            pos_count = (split_data[target_col] == 1).sum()\n",
    "            total_count = len(split_data)\n",
    "            conv_rate = pos_count / total_count if total_count > 0 else 0\n",
    "            \n",
    "            print(f\"   {split_name}:\")\n",
    "            print(f\"     Total: {total_count:,}\")\n",
    "            print(f\"     Positive: {pos_count:,} ({conv_rate:.1%})\")\n",
    "            print(f\"     Negative: {total_count - pos_count:,}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Ready for training! Use balanced dataset at: {balanced_output_dir}\")\n",
    "        print(f\"üí° To experiment with different ratios, change NEGATIVE_TO_POSITIVE_RATIO and re-run this cell\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No positive samples found - cannot create balanced dataset\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Target column '{target_col}' not found!\")\n",
    "    print(f\"Available columns: {list(df_processed.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a687f998-8905-42a4-bb92-d1f5244860b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean up memory\n",
    "import gc\n",
    "del df_processed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6461a96",
   "metadata": {},
   "source": [
    "You have  just created session-level features to train a session-based recommendation model using NVTabular. Now you can move to the the next notebook,`02-session-based-XLNet-with-PyT.ipynb` to train a session-based recommendation model using [XLNet](https://arxiv.org/abs/1906.08237), one of the state-of-the-art NLP model. Please shut down this kernel to free the GPU memory before you start the next one."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d795d7ca5d3ec3bd6293cc80853205a74ce23d484a2b8f537732a716747107c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
